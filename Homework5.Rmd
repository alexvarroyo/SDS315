---
title: "Homework 5"
author: "Alex Arroyo"
date: "2024-02-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(readr)
library(kableExtra)
letter_frequencies <- read_csv("letter_frequencies.csv")
brown_sentences<- readLines("~/Downloads/SDS/brown_sentences.txt")
```

### UTID: "aa87723"

Github: "<https://github.com/alexvarroyo/SDS315>"

## Iron Bank:

### Are Employees Violating Insider Trading Laws

In order to test this, we need to understand what the null hypothesis would be.

> The null Hypothesis: Trades are considered to be under review after 2.4% of trades are flagged. 

Test statistic: Number of flagged trades, in this case there were 70 out of 2021.

##### Assuming the null hypothesis is true, this is what the distribution of the test statistic.

```{r, echo= FALSE}
sim_trade = do(100000)*nflip(n=2021, prob=0.024)
ggplot(sim_trade) + 
  geom_histogram(aes(x=nflip),binwidth =1, fill = "lightblue") + geom_vline(xintercept = 70, color = "red", linetype = "solid")
```

In order to test the probability that 70 out of 2021 of the trades were flagged we look at the simulation of what the distribution would be of flagged trades given that the null hypothesis is true. Now to see the proportion of the simulations that were 70 flagged or above

```{r, echo=FALSE}
sum(sim_trade >= 70)/100000
```

The proportion of trades that would be flagged 70 times or more would be 0.00208, or 0.21%. As for the p-value, the smaller the number, the harder for the data to be explained by the null hypothesis. Based on the idea that is p is less than or equal to 0.05, then it makes the hypothesis significant, then we could say that since p= 0.00208 which is less than 0.05, the chances that 70 out of 2021 trades are flagged is significant and can not merely be due to chance. Therefore, in terms of what the data is referencing, I would conclude that there was indeed suspicion activity regarding Insider Trader Laws.

## Health Inspection
### Is the restaurant chain, Gourmet Bites, receiving too many violations. 

> Null hypothesis: On average, restaurants in the city are cited for health code violations at a 3% baseline. 

Test Statistic: Gourmet Bites received 8 violations out of 50 inspections. 
```{r, echo= FALSE}
sim_health= do(100000)*nflip(n=50, prob=0.03)

ggplot(sim_health) + 
  geom_histogram(aes(x=nflip),binwidth =1, fill = "lightblue") + geom_vline(xintercept = 8, color = "red", linetype = "solid")
sum(sim_health >= 8)/ 100000
```
 The p value would be calculated by finding the proportion of data from the simulation that is greater than or equal to the test statistic, 8. Which was 9e-05, a very very small number/proportion.
  Using the idea consistent that significance would be when a p value is less than 0.05, that would mean that the test statistic is an anomaly. Therefore, the restaurant Gourmet Bites, should be under investigation for its abnormal amount of health code violations. 
  
## LLM Water Marking 
What does the chi-squared statistic look like across lots of normal English sentences not generated by an LLM?

```{r, echo= FALSE}
 brown_sentences<- data.frame(data=brown_sentences)

calculate_chi_squared = function(sentence, freq_table) {
  
  # Ensure letter frequencies are normalized and sum to 1
  freq_table$Probability = freq_table$Probability / sum(freq_table$Probability)
  
  # Remove non-letters and convert to uppercase
  clean_sentence = gsub("[^A-Za-z]", "", sentence)
  clean_sentence = toupper(clean_sentence)
  
  # Count the occurrences of each letter in the sentence
  observed_counts = table(factor(strsplit(clean_sentence, "")[[1]], levels = freq_table$Letter))
  
  # Calculate expected counts
  total_letters = sum(observed_counts)
  expected_counts = total_letters * freq_table$Probability
  
  # Chi-squared statistic
  chi_squared_stat = sum((observed_counts - expected_counts)^2 / expected_counts)
  
 return( chi_squared_stat)
}


result_list <- apply(brown_sentences, 1, function(row) calculate_chi_squared(row["data"], letter_frequencies))
result_df <- data.frame(
  sentence = brown_sentences$data,
  max_chi_squared_stat = sapply(result_list, max)
)

ggplot(result_df) + 
  geom_histogram(aes(x=max_chi_squared_stat),fill='lightpink')
mean(result_df$max_chi_squared_stat)

```
In order to get the chi-squared statistic, I used a function that would " cleanup" the sentences so all the letters were uppercase, removed spaces, and the counted the frequency of the letters. This would then allow me to compare the sentences to the expected letter frequencies so I could then calculate the chi square statistic and see what the distribution of chi squares looks like across sentences not generated by a LLM. 

### Part B
```{r, echo=FALSE}
test_sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations"
)
test_sentences<- data.frame(data=test_sentences)
names(test_sentences) = c("data")

chi_squared_stats <- sapply(test_sentences$data, function(sentence) calculate_chi_squared(sentence, letter_frequencies))

# Create a data frame with the sentences and chi-squared statistics
test_results <- data.frame(
  test_sentence = test_sentences$data,
  chi_squared_stat = chi_squared_stats
)
test_results$Sent_number<- 1:10

ggplot(test_results) + geom_col(aes(x=as.factor(test_results$Sent_number), y= test_results$chi_squared_stat)) + labs(title= "Chi square statistics across tested senteces", x= "sentence number", y= "Calcualted chi Square") +  scale_x_discrete(labels = 1:10)

which.max(test_results$chi_squared_stat)
```
#### Using the Null distribution in order to figure out which sentence was created a LLM 

In order to figure out the chi-square I put the sentences into a dataset, and then applied the same function that I used to find the chi-squares for the sentences from Brown. This then gave me the chi-squares for these new sentences, which would be used to find the p-value. 


Sentence 6: "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland." 
This sentence had a much larger chi squared than what is expected when taking into account the average deviation for sentences in the English language. 


#### Table of P-values
```{r, echo=FALSE}
p_value_df <- data.frame(test_results)
p_value_df <- p_value_df[, -1]
p_value_df$p_value <- NA

for (i in 1:nrow(p_value_df)) {
  # Calculate the proportion using the given code
  p_value_df$p_value[i] <- sum(result_df$max_chi_squared_stat >= p_value_df$chi_squared_stat[i]) / nrow(result_df)
}
p_value_df$p_value <- round(p_value_df$p_value, 3)

p_value_df %>%
  kbl() %>%
  kable_styling()

```

#### Conclusion 
  Here in order to find the P-values I created a for-loop, that looped through each chi-square value, and found the p value by finding the proportion of values in the null distribution that were equal to or greater than each of the calculated chi-squares. Then to see which one was the sentences created by a LLM, I saw which one had a p value of 0.05 or less, which only one sentence did which was sentence 6. 


